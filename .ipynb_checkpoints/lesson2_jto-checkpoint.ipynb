{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division,print_function\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt\n",
    "from importlib import reload\n",
    "import utils; reload(utils)\n",
    "from utils import plots, get_batches, plot_confusion_matrix, get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import random, permutation\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.preprocessing import image\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]='PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = random((30,2))\n",
    "y = np.dot(x, [2., 3.]) + 1.\n",
    "model = Sequential([Dense(1, input_shape=(2,))])\n",
    "model.compile(optimizer=SGD(lr=0.1), loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.7325,  0.8787],\n",
       "        [ 0.5612,  0.1121],\n",
       "        [ 0.4768,  0.2547],\n",
       "        [ 0.9237,  0.5609],\n",
       "        [ 0.6825,  0.176 ]]),\n",
       " array([ 5.1012,  2.4586,  2.7176,  4.5303,  2.8931]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:5], y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.505111694335938"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x, y, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "30/30 [==============================] - 0s - loss: 1.4882     \n",
      "Epoch 2/5\n",
      "30/30 [==============================] - 0s - loss: 0.1034     \n",
      "Epoch 3/5\n",
      "30/30 [==============================] - 0s - loss: 0.0648     \n",
      "Epoch 4/5\n",
      "30/30 [==============================] - 0s - loss: 0.0343     \n",
      "Epoch 5/5\n",
      "30/30 [==============================] - 0s - loss: 0.0206     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ca243e69e8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, nb_epoch=5, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0096755325794219971"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x, y, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.7638],\n",
       "        [ 2.7592]], dtype=float32), array([ 1.2596], dtype=float32)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding layer to VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"data/dogscats//\"\n",
    "model_path = path + 'models/'\n",
    "if not os.path.exists(model_path): os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 #use max that doesn't give mem error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import vgg16; reload(vgg16)\n",
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "model = vgg.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(dirname, gen=image.ImageDataGenerator(), shuffle=True, batch_size=4, class_mode='categorical',\n",
    "                target_size=(224,224)):\n",
    "    return gen.flow_from_directory(dirname, target_size=target_size,\n",
    "            class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)\n",
    "\n",
    "def get_data(path, target_size=(224,224)):\n",
    "    batches = get_batches(path, shuffle=False, batch_size=1, class_mode=None, target_size=target_size)\n",
    "    return np.concatenate([batches.next() for i in range(batches.nb_sample)])\n",
    "\n",
    "import bcolz\n",
    "def save_array(fname, arr): c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
    "def load_array(fname): return bcolz.open(fname)[:]\n",
    "\n",
    "def onehot(x): return np.array(OneHotEncoder().fit_transform(x.reshape(-1,1)).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab training and validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 23000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_batches = get_batches(path+'valid', shuffle=False, batch_size=1)\n",
    "trn_batches = get_batches(path+'train', shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_data = get_data(path+'valid')\n",
    "trn_data = get_data(path+'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(model_path+'train_data.bc', trn_data)\n",
    "save_array(model_path+'valid_data.bc', val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_data = load_array(model_path+'train_data.bc')\n",
    "val_data = load_array(model_path+'valid_data.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the classes and convert to onehot format, these will be our *labels*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_classes = val_batches.classes\n",
    "trn_classes = trn_batches.classes\n",
    "val_labels = onehot(val_classes)\n",
    "trn_labels = onehot(trn_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the 1000 ImageNet probabilities for each image, which we will use as our *features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_features = model.predict(trn_data, batch_size=batch_size)\n",
    "val_features = model.predict(val_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(model_path+'train_lastlayer_features.bc', trn_features)\n",
    "save_array(model_path+'valid_lastlayer_features.bc', val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_features = load_array(model_path+'train_lastlayer_features.bc')\n",
    "val_features = load_array(model_path+'valid_lastlayer_features.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to create our linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm = Sequential([Dense(2,activation='softmax',input_shape=(1000,))])\n",
    "lm.compile(optimizer=RMSprop(lr=0.1), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16 samples, validate on 8 samples\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 0s - loss: 0.4781 - acc: 1.0000 - val_loss: 0.6430 - val_acc: 0.7500\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s - loss: 0.4735 - acc: 1.0000 - val_loss: 0.6418 - val_acc: 0.8750\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s - loss: 0.4690 - acc: 1.0000 - val_loss: 0.6412 - val_acc: 0.8750\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s - loss: 0.4646 - acc: 1.0000 - val_loss: 0.6402 - val_acc: 0.8750\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s - loss: 0.4602 - acc: 1.0000 - val_loss: 0.6395 - val_acc: 0.8750\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s - loss: 0.4559 - acc: 1.0000 - val_loss: 0.6387 - val_acc: 0.8750\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s - loss: 0.4516 - acc: 1.0000 - val_loss: 0.6380 - val_acc: 0.8750\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s - loss: 0.4474 - acc: 1.0000 - val_loss: 0.6373 - val_acc: 0.8750\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s - loss: 0.4432 - acc: 1.0000 - val_loss: 0.6366 - val_acc: 0.8750\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s - loss: 0.4391 - acc: 1.0000 - val_loss: 0.6359 - val_acc: 0.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c019ad0be0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.optimizer.lr = 0.01\n",
    "lm.fit(trn_features, trn_labels, nb_epoch=10, batch_size=batch_size, \n",
    "      validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s\n",
      "8/8 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "preds = lm.predict_classes(val_features, batch_size=batch_size)\n",
    "probs = lm.predict_proba(val_features, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 1]\n",
      " [0 4]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAEmCAYAAADmw8JdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nK8BJ08ysACdNM7MC/j/F0kFqDhxH8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2c019a5e518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(val_classes, preds)\n",
    "plot_confusion_matrix(cm, val_batches.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop off last layer and add softmax-activated dense layer with two categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.pop()\n",
    "for layer in model.layers: layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put our preprocessed image data into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator()\n",
    "trn_batches = gen.flow(trn_data, trn_labels, batch_size)\n",
    "val_batches = gen.flow(val_data, val_labels, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_model(model, trn_batches, val_batches, nb_epoch=1):\n",
    "    model.fit_generator(trn_batches, samples_per_epoch=trn_batches.n, nb_epoch=nb_epoch, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up and fit model to image data. Actually calculates all previous layers (not just last one) in order to get inputs to pass to final layer (not sure why using fit_generator rather than fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = RMSprop(lr=0.01)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 303s - loss: 2.0638 - acc: 0.8475 - val_loss: 1.6581 - val_acc: 0.8815\n"
     ]
    }
   ],
   "source": [
    "fit_model(model, trn_batches, val_batches, nb_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 12s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6580952156162829, 0.88149999999999995]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_data, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'finetune2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training multiple Layers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can optimise multiple dense layers for a general improvement. Make sure to fine tune first to avoid having one layer with random weights (which will cause others to move away from ImageNet params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = model.layers\n",
    "first_dense_idx = [index for index,layer in enumerate(layers) if type(layer) is Dense][0]\n",
    "for layer in layers[first_dense_idx:]: layer.trainable=True #set first dense layer onwards to be trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't need to recompile model as not changing architecture, but can change learning rate to lower as model already partly optimised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 293s - loss: 1.8752 - acc: 0.8700 - val_loss: 1.5753 - val_acc: 0.8900\n"
     ]
    }
   ],
   "source": [
    "K.set_value(opt.lr, 0.01)\n",
    "fit_model(model, trn_batches, val_batches, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'finetune3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also fine tune some of the convolutional layers, though there is generally little room for improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in layers[12:]: layer.trainable=True\n",
    "K.set_value(opt.lr, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 289s - loss: 1.7681 - acc: 0.8791 - val_loss: 1.4504 - val_acc: 0.9025\n"
     ]
    }
   ],
   "source": [
    "fit_model(model, trn_batches, val_batches, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'finetune4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
