{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3333)\n",
    "#sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]='PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='2'\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dense, Lambda, Flatten, Dropout\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.optimizers import Adam, sgd\n",
    "from keras.layers import Input, Embedding, Reshape, merge, LSTM, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.preprocessing import image, sequence\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import bcolz\n",
    "\n",
    "#if 'session' in locals() and session is not None:\n",
    "#    print('Close interactive session')\n",
    "#    session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get IMDB reviews with labels, prepare for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'and', 'a', 'of', 'to', 'is', 'br', 'in', 'it', 'i']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get imdb dataset and sort the word index list by index/rank (already ranked for us!)\n",
    "from keras.datasets import imdb\n",
    "idx = imdb.get_word_index()\n",
    "idx_arr = sorted(idx, key=idx.get)\n",
    "idx_arr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import reviews directly as Jeremy doesn't like what Keras does with it when it imports it\n",
    "from keras.utils.data_utils import get_file\n",
    "path = get_file('imdb_full.pkl',\n",
    "                origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl',\n",
    "                md5_hash='d091312047c43cf9e4e38fef92437263')\n",
    "f = open(path, 'rb')\n",
    "(x_train, labels_train), (x_test, labels_test) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Build map from id to word\n",
    "3. Check out first review in id format and word format\n",
    "4. Check out training labels\n",
    "5. Reduce vocab size to 5000 most common words (replace rest with 5000th)\n",
    "6. Check out the distribution of word length\n",
    "7. Pad shorter reviews (keras - sequence.pad_sequences) with zeros or truncate longer reviews to bring all to 500 words long\n",
    "8. Training shape should now be (25000, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {idx[k]: k for k in idx.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23022, 309, 6, 3, 1069, 209, 9, 2175, 30, 1, 169, 55, 14, 46, 82, 5869, 41, 393, 110, 138, 14, 5359, 58, 4477, 150, 8, 1, 5032, 5948, 482, 69, 5, 261, 12, 23022, 73935, 2003, 6, 73, 2436, 5, 632, 71, 6, 5359, 1, 25279, 5, 2004, 10471, 1, 5941, 1534, 34, 67, 64, 205, 140, 65, 1232, 63526, 21145, 1, 49265, 4, 1, 223, 901, 29, 3024, 69, 4, 1, 5863, 10, 694, 2, 65, 1534, 51, 10, 216, 1, 387, 8, 60, 3, 1472, 3724, 802, 5, 3521, 177, 1, 393, 10, 1238, 14030, 30, 309, 3, 353, 344, 2989, 143, 130, 5, 7804, 28, 4, 126, 5359, 1472, 2375, 5, 23022, 309, 10, 532, 12, 108, 1470, 4, 58, 556, 101, 12, 23022, 309, 6, 227, 4187, 48, 3, 2237, 12, 9, 215'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join([str(idx) for idx in x_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell high's satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers' pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled at high a classic line inspector i'm here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isn't\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([idx2word[idx] for idx in x_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx) #number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "trn = [np.array(x) for x in x_train]\n",
    "for i in range(len(trn)): trn[i][trn[i]>vocab_size] = vocab_size\n",
    "test = [np.array(x) for x in x_test]\n",
    "for i in range(len(test)): test[i][test[i]>vocab_size] = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2493, 10, 237.71364)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array([len(x) for x in trn])\n",
    "(lens.max(), lens.min(), lens.mean()) #review length stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 500\n",
    "trn = sequence.pad_sequences(trn, maxlen=seq_len)\n",
    "test = sequence.pad_sequences(test, maxlen=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 500), (25000, 500))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(trn.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The \"simple\" model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First build a simple single hidden layer NN\n",
    "1. create a sequential model with layers: embedding, flatten, dense(100, relu), dropout(0.7), dense(1, sigmoid)\n",
    "2. compile model (binary crossentropy loss and adam optimizer with accuracy metric) and check summary\n",
    "3. fit model with batch size 64 and 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_factors = 32\n",
    "model = Sequential([Embedding(input_dim=vocab_size, output_dim=latent_factors, input_length=seq_len),\n",
    "                   Flatten(),\n",
    "                   Dense(output_dim=100, activation=\"relu\"),\n",
    "                   Dropout(0.9), #Model wants to overfit data after only 1 epoch so bumped this up with similar accuracy resulting\n",
    "                   Dense(output_dim=1, activation=\"sigmoid\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_8 (Embedding)          (None, 500, 32)       160000      embedding_input_7[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)              (None, 16000)         0           embedding_8[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_15 (Dense)                 (None, 100)           1600100     flatten_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 100)           0           dense_15[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_16 (Dense)                 (None, 1)             101         dropout_8[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1,760,201\n",
      "Trainable params: 1,760,201\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 3s - loss: 0.2301 - acc: 0.9123 - val_loss: 0.2914 - val_acc: 0.8804\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 3s - loss: 0.1665 - acc: 0.9383 - val_loss: 0.3335 - val_acc: 0.8756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22ec3b9ca90>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, batch_size=64, nb_epoch=2, validation_data=(test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional NN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a 1D CNN\n",
    "1. Sequential model of: embedding, dropout(0.2), conv1d(64,5), dropout(0.2), maxpooling1d, flatten, dense(100), dropout(0.7), dense(1)\n",
    "2. Compile (same as before)\n",
    "3. fit (4 epochs)\n",
    "\n",
    "Dropout in Embedding removes some of the latent factors, dropout afterwards removes some of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1 = Sequential([Embedding(input_dim=vocab_size, output_dim=latent_factors, input_length=seq_len, dropout=0.2),\n",
    "                   Dropout(0.3),\n",
    "                   Convolution1D(nb_filter=64, filter_length=5, border_mode=\"valid\", activation=\"relu\"),\n",
    "                   Dropout(0.3),\n",
    "                   MaxPooling1D(),\n",
    "                   Flatten(),\n",
    "                   Dense(100, activation=\"relu\"),\n",
    "                   Dropout(0.9),\n",
    "                   Dense(1, activation=\"sigmoid\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_11 (Embedding)         (None, 500, 32)       160000      embedding_input_10[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)             (None, 500, 32)       0           embedding_11[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_3 (Convolution1D)  (None, 496, 64)       10304       dropout_14[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)             (None, 496, 64)       0           convolution1d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_3 (MaxPooling1D)    (None, 248, 64)       0           dropout_15[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)             (None, 15872)         0           maxpooling1d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_19 (Dense)                 (None, 100)           1587300     flatten_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)             (None, 100)           0           dense_19[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_20 (Dense)                 (None, 1)             101         dropout_16[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,757,705\n",
      "Trainable params: 1,757,705\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv1.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])\n",
    "conv1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jtownend\\Anaconda3\\envs\\windl\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 6s - loss: 0.6467 - acc: 0.5698 - val_loss: 0.3665 - val_acc: 0.8661\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 6s - loss: 0.3913 - acc: 0.8369 - val_loss: 0.2947 - val_acc: 0.8835\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 5s - loss: 0.3282 - acc: 0.8680 - val_loss: 0.2777 - val_acc: 0.8940\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 5s - loss: 0.3108 - acc: 0.8790 - val_loss: 0.2736 - val_acc: 0.8896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22f25518ac8>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.fit(trn, labels_train, batch_size=64, nb_epoch=4, validation_data=(test, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1.optimizer.lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 6s - loss: 0.2864 - acc: 0.8871 - val_loss: 0.2644 - val_acc: 0.8942\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 5s - loss: 0.2741 - acc: 0.8919 - val_loss: 0.2661 - val_acc: 0.8928\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 5s - loss: 0.2703 - acc: 0.8974 - val_loss: 0.2695 - val_acc: 0.8882\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 5s - loss: 0.2591 - acc: 0.8983 - val_loss: 0.2617 - val_acc: 0.8918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22f29841198>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.fit(trn, labels_train, batch_size=64, nb_epoch=4, validation_data=(test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-trained vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use pre-trained embedding values from glove with the previous CNN, and BOOM, accuracy increase!\n",
    "1. get the weights\n",
    "2. unpack weights\n",
    "3. glove uses different idx2word dict than imdb, so need to match "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_dataset(dataset):\n",
    "    \"\"\"Download the requested glove dataset from files.fast.ai\n",
    "    and return a location that can be passed to load_vectors.\n",
    "    \"\"\"\n",
    "    # see wordvectors.ipynb for info on how these files were\n",
    "    # generated from the original glove data.\n",
    "    md5sums = {'6B.50d': '8e1557d1228decbda7db6dfd81cd9909',\n",
    "               '6B.100d': 'c92dbbeacde2b0384a43014885a60b2c',\n",
    "               '6B.200d': 'af271b46c04b0b2e41a84d8cd806178d',\n",
    "               '6B.300d': '30290210376887dcc6d0a5a6374d8255'}\n",
    "    glove_path = os.path.abspath('data/glove/results')\n",
    "    %mkdir -p $glove_path\n",
    "    return get_file(dataset,\n",
    "                    'http://files.fast.ai/models/glove/' + dataset + '.tgz',\n",
    "                    cache_subdir=glove_path,\n",
    "                    md5_hash=md5sums.get(dataset, None),\n",
    "                    untar=True)\n",
    "\n",
    "def load_vectors(loc):\n",
    "    return (bcolz.open(loc+'.dat'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb')),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file -p already exists.\n",
      "Error occurred while processing: -p.\n",
      "A subdirectory or file D:\\jtownend\\fast.ai\\data\\glove\\results already exists.\n",
      "Error occurred while processing: D:\\jtownend\\fast.ai\\data\\glove\\results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untaring file...\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-167-bdad4353bb30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvecs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordidx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_glove_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'6B.50d'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-166-3378ee4381a9>\u001b[0m in \u001b[0;36mload_vectors\u001b[1;34m(loc)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     return (bcolz.open(loc+'.dat'),\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_words.pkl'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         pickle.load(open(loc+'_idx.pkl','rb')))\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "vecs, words, wordidx = load_vectors(get_glove_dataset('6B.50d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file -p already exists.\n",
      "Error occurred while processing: -p.\n",
      "A subdirectory or file D:\\jtownend\\fast.ai\\data\\glove\\results already exists.\n",
      "Error occurred while processing: D:\\jtownend\\fast.ai\\data\\glove\\results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untaring file...\n"
     ]
    }
   ],
   "source": [
    "vecs = bcolz.open(get_glove_dataset('6B.50d')+'.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file -p already exists.\n",
      "Error occurred while processing: -p.\n",
      "A subdirectory or file D:\\jtownend\\fast.ai\\data\\glove\\results already exists.\n",
      "Error occurred while processing: D:\\jtownend\\fast.ai\\data\\glove\\results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untaring file...\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "unknown encoding: bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-187-c9d5d12496a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_glove_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'6B.50d'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_words.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bytes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m: unknown encoding: bytes"
     ]
    }
   ],
   "source": [
    "words = pickle.load(open(get_glove_dataset('6B.50d')+'_words.pkl', encoding='bytes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file -p already exists.\n",
      "Error occurred while processing: -p.\n",
      "A subdirectory or file D:\\jtownend\\fast.ai\\data\\glove\\results already exists.\n",
      "Error occurred while processing: D:\\jtownend\\fast.ai\\data\\glove\\results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untaring file...\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xc5 in position 3: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-171-1bbdfa200cbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwordidx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_glove_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'6B.50d'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_idx.pkl'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xc5 in position 3: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "wordidx = pickle.load(open(get_glove_dataset('6B.50d')+'_idx.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
